"""
Script simples para testar a funcionalidade LLM.
"""

from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional
import os
from openai import OpenAI

# Criar app FastAPI
app = FastAPI(title="Teste LLM - Qualidade do Ar")

# Inicializar cliente OpenAI
client = OpenAI()


class AirQualityAnalysisRequest(BaseModel):
    """Request para anÃ¡lise de qualidade do ar."""
    pm25: float
    pm10: float
    city: str
    temperature: Optional[float] = None
    humidity: Optional[float] = None


class AirQualityAnalysisResponse(BaseModel):
    """Response da anÃ¡lise de qualidade do ar."""
    analysis: str
    recommendations: str


@app.get("/")
async def root():
    """Endpoint raiz."""
    return {
        "message": "API de Teste - AnÃ¡lise de Qualidade do Ar com LLM",
        "endpoints": {
            "test": "/test",
            "analyze": "/analyze"
        }
    }


@app.get("/test")
async def test_llm():
    """
    Endpoint de teste para verificar se o LLM estÃ¡ funcionando.
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=[
                {
                    "role": "user",
                    "content": "Responda apenas 'OK - LLM funcionando!' se vocÃª estÃ¡ operacional."
                }
            ],
            max_tokens=20
        )
        
        return {
            "status": "success",
            "message": "LLM estÃ¡ funcionando corretamente",
            "response": response.choices[0].message.content,
            "model": response.model
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"Erro ao testar LLM: {str(e)}"
        }


@app.post("/analyze", response_model=AirQualityAnalysisResponse)
async def analyze_air_quality(request: AirQualityAnalysisRequest):
    """
    Analisa dados de qualidade do ar usando LLM e retorna recomendaÃ§Ãµes.
    """
    try:
        # Construir prompt com os dados
        prompt = f"""Analise os seguintes dados de qualidade do ar para a cidade de {request.city}:

- PM2.5: {request.pm25} Âµg/mÂ³
- PM10: {request.pm10} Âµg/mÂ³"""

        if request.temperature is not None:
            prompt += f"\n- Temperatura: {request.temperature}Â°C"
        
        if request.humidity is not None:
            prompt += f"\n- Umidade: {request.humidity}%"

        prompt += """

Por favor, forneÃ§a:
1. Uma anÃ¡lise breve sobre a qualidade do ar (2-3 frases)
2. RecomendaÃ§Ãµes prÃ¡ticas para a populaÃ§Ã£o (2-3 itens)

Seja direto e objetivo."""

        # Chamar LLM
        response = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=[
                {
                    "role": "system",
                    "content": "VocÃª Ã© um especialista em qualidade do ar e saÃºde pÃºblica. ForneÃ§a anÃ¡lises claras e recomendaÃ§Ãµes prÃ¡ticas baseadas nos dados de poluiÃ§Ã£o do ar."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.7,
            max_tokens=300
        )
        
        # Extrair resposta
        llm_response = response.choices[0].message.content
        
        # Separar anÃ¡lise e recomendaÃ§Ãµes
        parts = llm_response.split("\n\n")
        
        if len(parts) >= 2:
            analysis = parts[0]
            recommendations = "\n\n".join(parts[1:])
        else:
            analysis = llm_response
            recommendations = "Consulte as autoridades de saÃºde locais para mais informaÃ§Ãµes."
        
        return AirQualityAnalysisResponse(
            analysis=analysis.strip(),
            recommendations=recommendations.strip()
        )
        
    except Exception as e:
        return AirQualityAnalysisResponse(
            analysis=f"Erro ao processar anÃ¡lise: {str(e)}",
            recommendations="NÃ£o foi possÃ­vel gerar recomendaÃ§Ãµes."
        )


if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ Iniciando servidor de teste LLM...")
    print("ğŸ“ Acesse: http://localhost:8000")
    print("ğŸ“š DocumentaÃ§Ã£o: http://localhost:8000/docs")
    uvicorn.run(app, host="0.0.0.0", port=8000)
